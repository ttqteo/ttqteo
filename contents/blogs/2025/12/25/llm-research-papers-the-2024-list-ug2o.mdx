---
title: "LLM Research Papers: The 2024 List"
description: "archive from: https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one"
date: "2025-12-24"
authors:
  - avatar: ""
    handle: "ttqteo"
    username: "ttqteo"
    handleUrl: "https://github.com/ttqteo"
cover: ""
isPublished: true
tags: ""
---

It’s been a very eventful and exciting year in AI research. This is especially true if you are interested in LLMs.


I had big plans for this December edition and was planning to publish a new article with a discussion of all my research highlights from 2024. I still plan to do so, but due to an accident and serious injury, I am currently unable to work at a computer and finish the draft. But I hope to recover in the upcoming weeks and be back on my feet soon.


In the meantime, I want to share my running bookmark list of many fascinating (mostly LLM-related) papers I stumbled upon in 2024. It’s just a list, but maybe it will come in handy for those who are interested in finding some gems to read for the holidays.


And if you are interested in more code-heavy reading and tinkering, <a target="_blank" rel="" href="https://amzn.to/4fqvn0D"><u>My Build A Large Language Model (From Scratch)</u></a> book is out on Amazon as of last month.


In addition, I added a lot of bonus materials to the <a target="_blank" rel="" href="https://github.com/rasbt/LLMs-from-scratch"><u>GitHub repository</u></a>.
<img class="rounded-lg border my-4" src="https://substackcdn.com/image/fetch/$s_!WXSG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg" alt="" width="1456" height="1861">
<a target="_blank" rel="" href="https://github.com/rasbt/LLMs-from-scratch"><u>Bonus materials in the GitHub repository</u></a> (stars highlight my personal favorites)

Thanks for your understanding and support, and I hope to make a full recovery soon and be back with the Research Highlights 2024 article in a few weeks!

Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.

**Subscribe**


## **January 2024**

- 
1 Jan, *Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.00788"><u>https://arxiv.org/abs/2401.00788</u></a>

- 
2 Jan, *A Comprehensive Study of Knowledge Editing for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.01286"><u>https://arxiv.org/abs/2401.01286</u></a>

- 
2 Jan, *LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.01325"><u>https://arxiv.org/abs/2401.01325</u></a>

- 
2 Jan, *Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.01335"><u>https://arxiv.org/abs/2401.01335</u></a>

- 
2 Jan, *LLaMA Beyond English: An Empirical Study on Language Capability Transfer*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.01055"><u>https://arxiv.org/abs/2401.01055</u></a>

- 
3 Jan, *A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.01967"><u>https://arxiv.org/abs/2401.01967</u></a>

- 
4 Jan, *LLaMA Pro: Progressive LLaMA with Block Expansion*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.02415"><u>https://arxiv.org/abs/2401.02415</u></a>

- 
4 Jan, *LLM Augmented LLMs: Expanding Capabilities through Composition*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.02412"><u>https://arxiv.org/abs/2401.02412</u></a>

- 
4 Jan, *Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.02994"><u>https://arxiv.org/abs/2401.02994</u></a>

- 
5 Jan, *DeepSeek LLM: Scaling Open-Source Language Models with Longtermism*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.02954"><u>https://arxiv.org/abs/2401.02954</u></a>

- 
5 Jan, *Denoising Vision Transformers*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.02957"><u>https://arxiv.org/abs/2401.02957</u></a>

- 
7 Jan, *Soaring from 4K to 400K: Extending LLM’s Context with Activation Beacon*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.03462"><u>https://arxiv.org/abs/2401.03462</u></a>

- 
8 Jan, *Mixtral of Experts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.04088"><u>https://arxiv.org/abs/2401.04088</u></a>

- 
8 Jan, *MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.04081"><u>https://arxiv.org/abs/2401.04081</u></a>

- 
8 Jan, *A Minimaximalist Approach to Reinforcement Learning from Human Feedback*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.04056"><u>https://arxiv.org/abs/2401.04056</u></a>

- 
9 Jan, *RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.04679"><u>https://arxiv.org/abs/2401.04679</u></a>

- 
10 Jan, *Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.05566"><u>https://arxiv.org/abs/2401.05566</u></a>

- 
11 Jan, *Transformers are Multi-State RNNs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.06104"><u>https://arxiv.org/abs/2401.06104</u></a>

- 
11 Jan, *A Closer Look at AUROC and AUPRC under Class Imbalance*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.06091"><u>https://arxiv.org/abs/2401.06091</u></a>

- 
12 Jan, *An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.06692"><u>https://arxiv.org/abs/2401.06692</u></a>

- 
16 Jan, *Tuning Language Models by Proxy*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.08565"><u>https://arxiv.org/abs/2401.08565</u></a>

- 
16 Jan, *Scalable Pre-training of Large Autoregressive Image Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.08541"><u>https://arxiv.org/abs/2401.08541</u></a>

- 
16 Jan, *Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.08500"><u>https://arxiv.org/abs/2401.08500</u></a>

- 
16 Jan, *RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.08406"><u>https://arxiv.org/abs/2401.08406</u></a>

- 
17 Jan, *ReFT: Reasoning with Reinforced Fine-Tuning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.08967"><u>https://arxiv.org/abs/2401.08967</u></a>

- 
18 Jan, *DiffusionGPT: LLM-Driven Text-to-Image Generation System*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.10061"><u>https://arxiv.org/abs/2401.10061</u></a>

- 
18 Jan, *Self-Rewarding Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.10020"><u>https://arxiv.org/abs/2401.10020</u></a>

- 
18 Jan, *VMamba: Visual State Space Model*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.10166"><u>https://arxiv.org/abs/2401.10166</u></a>

- 
19 Jan, *Knowledge Fusion of Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.10491"><u>https://arxiv.org/abs/2401.10491</u></a>

- 
22 Jan, *SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.12168"><u>https://arxiv.org/abs/2401.12168</u></a>

- 
22 Jan, *WARM: On the Benefits of Weight Averaged Reward Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.12187"><u>https://arxiv.org/abs/2401.12187</u></a>

- 
22 Jan, *Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.12070"><u>https://arxiv.org/abs/2401.12070</u></a>

- 
24 Jan, *MambaByte: Token-free Selective State Space Model*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.13660"><u>https://arxiv.org/abs/2401.13660</u></a>

- 
24 Jan, *SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.13160"><u>https://arxiv.org/abs/2401.13160</u></a>

- 
25 Jan, *Rethinking Patch Dependence for Masked Autoencoders*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.14391"><u>https://arxiv.org/abs/2401.14391</u></a>

- 
25 Jan, *Pix2gestalt: Amodal Segmentation by Synthesizing Wholes*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.14398"><u>https://arxiv.org/abs/2401.14398</u></a>

- 
25 Jan, *Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.14405"><u>https://arxiv.org/abs/2401.14405</u></a>

- 
26 Jan, *EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.15077"><u>https://arxiv.org/abs/2401.15077</u></a>

- 
29 Jan, *MoE-LLaVA: Mixture of Experts for Large Vision-Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.15947"><u>https://arxiv.org/abs/2401.15947</u></a>

- 
29 Jan, *Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.16380"><u>https://arxiv.org/abs/2401.16380</u></a>

- 
31 Jan, *KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.18079"><u>https://arxiv.org/abs/2401.18079</u></a>



## **February 2024**

- 
1 Feb, *Efficient Exploration for LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.00396"><u>https://arxiv.org/abs/2402.00396</u></a>

- 
1 Feb, *OLMo: Accelerating the Science of Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.00838"><u>https://arxiv.org/abs/2402.00838</u></a>

- 
1 Feb, *Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.00841"><u>https://arxiv.org/abs/2402.00841</u></a>

- 
1 Feb, *Repeat After Me: Transformers are Better than State Space Models at Copying*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.01032"><u>https://arxiv.org/abs/2402.01032</u></a>

- 
2 Feb, *LiPO: Listwise Preference Optimization through Learning-to-Rank*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.01878"><u>https://arxiv.org/abs/2402.01878</u></a>

- 
2 Feb, *FindingEmo: An Image Dataset for Emotion Recognition in the Wild*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.01355"><u>https://arxiv.org/abs/2402.01355</u></a>

- 
3 Feb, *More Agents Is All You Need*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.05120"><u>https://arxiv.org/abs/2402.05120</u></a>

- 
5 Feb, *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.03300"><u>https://arxiv.org/abs/2402.03300</u></a>

- 
6 Feb, *MobileVLM V2: Faster and Stronger Baseline for Vision Language Model*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.03766"><u>https://arxiv.org/abs/2402.03766</u></a>

- 
6 Feb, *A Phase Transition Between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.03902"><u>https://arxiv.org/abs/2402.03902</u></a>

- 
6 Feb, *Scaling Laws for Downstream Task Performance of Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.04177"><u>https://arxiv.org/abs/2402.04177</u></a>

- 
6 Feb, *MOMENT: A Family of Open Time-series Foundation Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.03885"><u>https://arxiv.org/abs/2402.03885</u></a>

- 
6 Feb, *Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.03749"><u>https://arxiv.org/abs/2402.03749</u></a>

- 
6 Feb, *Self-Discover: Large Language Models Self-Compose Reasoning Structures*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.03620"><u>https://arxiv.org/abs/2402.03620</u></a>

- 
7 Feb, *Grandmaster-Level Chess Without Search*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.04494"><u>https://arxiv.org/abs/2402.04494</u></a>

- 
7 Feb, *Direct Language Model Alignment from Online AI Feedback*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.04792"><u>https://arxiv.org/abs/2402.04792</u></a>

- 
8 Feb, *Buffer Overflow in Mixture of Experts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.05526"><u>https://arxiv.org/abs/2402.05526</u></a>

- 
9 Feb, *The Boundary of Neural Network Trainability is Fractal*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.06184"><u>https://arxiv.org/abs/2402.06184</u></a>

- 
11 Feb, *ODIN: Disentangled Reward Mitigates Hacking in RLHF*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.07319"><u>https://arxiv.org/abs/2402.07319</u></a>

- 
12 Feb, *Policy Improvement using Language Feedback Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.07876"><u>https://arxiv.org/abs/2402.07876</u></a>

- 
12 Feb, *Scaling Laws for Fine-Grained Mixture of Experts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.07871"><u>https://arxiv.org/abs/2402.07871</u></a>

- 
12 Feb, *Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.07610"><u>https://arxiv.org/abs/2402.07610</u></a>

- 
12 Feb, *Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.07610"><u>https://arxiv.org/abs/2402.07610</u></a>

- 
12 Feb, *Suppressing Pink Elephants with Direct Principle Feedback*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.07896"><u>https://arxiv.org/abs/2402.07896</u></a>

- 
13 Feb, *World Model on Million-Length Video And Language With RingAttention*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.08268"><u>https://arxiv.org/abs/2402.08268</u></a>

- 
13 Feb, *Mixtures of Experts Unlock Parameter Scaling for Deep RL*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.08609"><u>https://arxiv.org/abs/2402.08609</u></a>

- 
14 Feb, *DoRA: Weight-Decomposed Low-Rank Adaptation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.09353"><u>https://arxiv.org/abs/2402.09353</u></a>

- 
14 Feb, *Transformers Can Achieve Length Generalization But Not Robustly*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.09371"><u>https://arxiv.org/abs/2402.09371</u></a>

- 
15 Feb, *BASE TTS: Lessons From Building a Billion-Parameter Text-to-Speech Model on 100K Hours of Data*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.08093"><u>https://arxiv.org/abs/2402.08093</u></a>

- 
15 Feb, *Recovering the Pre-Fine-Tuning Weights of Generative Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.10208"><u>https://arxiv.org/abs/2402.10208</u></a>

- 
15 Feb, *Generative Representational Instruction Tuning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.09906"><u>https://arxiv.org/abs/2402.09906</u></a>

- 
16 Feb, *FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.10986"><u>https://arxiv.org/abs/2402.10986</u></a>

- 
17 Feb, *OneBit: Towards Extremely Low-bit Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.11295"><u>https://arxiv.org/abs/2402.11295</u></a>

- 
18 Feb, *LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.11550"><u>https://arxiv.org/abs/2402.11550</u></a>

- 
19 Feb, *Reformatted Alignment*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.12219"><u>https://arxiv.org/abs/2402.12219</u></a>

- 
19 Feb, *AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.12226"><u>https://arxiv.org/abs/2402.12226</u></a>

- 
19 Feb, *Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.12030"><u>https://arxiv.org/abs/2402.12030</u></a>

- 
19 Feb, *LoRA+: Efficient Low Rank Adaptation of Large Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.12354"><u>https://arxiv.org/abs/2402.12354</u></a>

- 
20 Feb, *Neural Network Diffusion*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.13144"><u>https://arxiv.org/abs/2402.13144</u></a>

- 
21 Feb, *YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.13616"><u>https://arxiv.org/abs/2402.13616</u></a>

- 
21 Feb, *LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.13753"><u>https://arxiv.org/abs/2402.13753</u></a>

- 
21 Feb, *Large Language Models for Data Annotation: A Survey*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.13446"><u>https://arxiv.org/abs/2402.13446</u></a>

- 
22 Feb, *TinyLLaVA: A Framework of Small-scale Large Multimodal Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.14289"><u>https://arxiv.org/abs/2402.14289</u></a>

- 
22 Feb, *Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.14740"><u>https://arxiv.org/abs/2402.14740</u></a>

- 
23 Feb, *Genie: Generative Interactive Environments*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.15391"><u>https://arxiv.org/abs/2402.15391</u></a>

- 
26 Feb, *CARTE: Pretraining and Transfer for Tabular Learning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.16785"><u>https://arxiv.org/abs/2402.16785</u></a>

- 
27 Feb, *The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.17764"><u>https://arxiv.org/abs/2402.17764</u></a>

- 
27 Feb, *Sora Generates Videos with Stunning Geometrical Consistency*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.17403"><u>https://arxiv.org/abs/2402.17403</u></a>

- 
27 Feb, *When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.17193"><u>https://arxiv.org/abs/2402.17193</u></a>

- 
29 Feb, *Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2402.19427"><u>https://arxiv.org/abs/2402.19427</u></a>



## **March 2024**

- 
1 Mar, *Learning and Leveraging World Models in Visual Representation Learning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.00504"><u>https://arxiv.org/abs/2403.00504</u></a>

- 
3 Mar, *Improving LLM Code Generation with Grammar Augmentation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.01632"><u>https://arxiv.org/abs/2403.01632</u></a>

- 
3 Mar, *The Hidden Attention of Mamba Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.01590"><u>https://arxiv.org/abs/2403.01590</u></a>

- 
4 Mar, *Training-Free Pretrained Model Merging*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.01753"><u>https://arxiv.org/abs/2403.01753</u></a>

- 
4 Mar, *Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.02308"><u>https://arxiv.org/abs/2403.02308</u></a>

- 
5 Mar, *The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03218"><u>https://arxiv.org/abs/2403.03218</u></a>

- 
5 Mar, *Evolution Transformer: In-Context Evolutionary Optimization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.02985"><u>https://arxiv.org/abs/2403.02985</u></a>

- 
5 Mar, *Enhancing Vision-Language Pre-training with Rich Supervisions*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03346"><u>https://arxiv.org/abs/2403.03346</u></a>

- 
5 Mar, *Scaling Rectified Flow Transformers for High-Resolution Image Synthesis*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03206"><u>https://arxiv.org/abs/2403.03206</u></a>

- 
5 Mar, *Design2Code: How Far Are We From Automating Front-End Engineering?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03163"><u>https://arxiv.org/abs/2403.03163</u></a>

- 
6 Mar, *ShortGPT: Layers in Large Language Models are More Redundant Than You Expect*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03853"><u>https://arxiv.org/abs/2403.03853</u></a>

- 
6 Mar, *Backtracing: Retrieving the Cause of the Query*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03956"><u>https://arxiv.org/abs/2403.03956</u></a>

- 
6 Mar, *Learning to Decode Collaboratively with Multiple Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03870"><u>https://arxiv.org/abs/2403.03870</u></a>

- 
6 Mar, *SaulLM-7B: A pioneering Large Language Model for Law*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03883"><u>https://arxiv.org/abs/2403.03883</u></a>

- 
6 Mar, *Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03864"><u>https://arxiv.org/abs/2403.03864</u></a>

- 
6 Mar, *3D Diffusion Policy*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03954"><u>https://arxiv.org/abs/2403.03954</u></a>

- 
6 Mar, *MedMamba: Vision Mamba for Medical Image Classification*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03849"><u>https://arxiv.org/abs/2403.03849</u></a>

- 
6 Mar, *GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03507"><u>https://arxiv.org/abs/2403.03507</u></a>

- 
6 Mar, *Stop Regressing: Training Value Functions via Classification for Scalable Deep RL*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.03950"><u>https://arxiv.org/abs/2403.03950</u></a>

- 
7 Mar, *How Far Are We from Intelligent Visual Deductive Reasoning?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.04732"><u>https://arxiv.org/abs/2403.04732</u></a>

- 
7 Mar, *Common 7B Language Models Already Possess Strong Math Capabilities*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.04706"><u>https://arxiv.org/abs/2403.04706</u></a>

- 
8 Mar, *Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.05530"><u>https://arxiv.org/abs/2403.05530</u></a>

- 
8 Mar, *Is Cosine-Similarity of Embeddings Really About Similarity?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.05440"><u>https://arxiv.org/abs/2403.05440</u></a>

- 
8 Mar, *LLM4Decompile: Decompiling Binary Code with Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.05286"><u>https://arxiv.org/abs/2403.05286</u></a>

- 
9 Mar, *Algorithmic Progress in Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.05812"><u>https://arxiv.org/abs/2403.05812</u></a>

- 
11 Mar, *Stealing Part of a Production Language Model*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.06634"><u>https://arxiv.org/abs/2403.06634</u></a>

- 
12 Mar, *Chronos: Learning the Language of Time Series*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.07815"><u>https://arxiv.org/abs/2403.07815</u></a>

- 
13 Mar, *Simple and Scalable Strategies to Continually Pre-train Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.08763"><u>https://arxiv.org/abs/2403.08763</u></a>

- 
13 Mar, *Language Models Scale Reliably With Over-Training and on Downstream Tasks*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.08540"><u>https://arxiv.org/abs/2403.08540</u></a>

- 
14 Mar, *BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.09347"><u>https://arxiv.org/abs/2403.09347</u></a>

- 
14 Mar, *LocalMamba: Visual State Space Model with Windowed Selective Scan*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.09338"><u>https://arxiv.org/abs/2403.09338</u></a>

- 
14 Mar, *GiT: Towards Generalist Vision Transformer through Universal Language Interface*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.09394"><u>https://arxiv.org/abs/2403.09394</u></a>

- 
14 Mar, *MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.09611"><u>https://arxiv.org/abs/2403.09611</u></a>

- 
15 Mar, *RAFT: Adapting Language Model to Domain Specific RAG*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.10131"><u>https://arxiv.org/abs/2403.10131</u></a>

- 
18 Mar, *TnT-LLM: Text Mining at Scale with Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.12173"><u>https://arxiv.org/abs/2403.12173</u></a>

- 
18 Mar, *Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.15447"><u>https://arxiv.org/abs/2403.15447</u></a>

- 
19 Mar, *PERL: Parameter Efficient Reinforcement Learning from Human Feedback*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.10704"><u>https://arxiv.org/abs/2403.10704</u></a>

- 
20 Mar, *RewardBench: Evaluating Reward Models for Language Modeling*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.13787"><u>https://arxiv.org/abs/2403.13787</u></a>

- 
20 Mar, *LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.13372"><u>https://arxiv.org/abs/2403.13372</u></a>

- 
21 Mar, *RakutenAI-7B: Extending Large Language Models for Japanese*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.15484"><u>https://arxiv.org/abs/2403.15484</u></a>

- 
22 Mar, *SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time Series*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.15360"><u>https://arxiv.org/abs/2403.15360</u></a>

- 
22 Mar, *Can Large Language Models Explore In-Context?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.15371"><u>https://arxiv.org/abs/2403.15371</u></a>

- 
22 Mar, *LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.15042"><u>https://arxiv.org/abs/2403.15042</u></a>

- 
25 Mar, *LLM Agent Operating System*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.16971"><u>https://arxiv.org/abs/2403.16971</u></a>

- 
26 Mar, *The Unreasonable Ineffectiveness of the Deeper Layers*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.17887"><u>https://arxiv.org/abs/2403.17887</u></a>

- 
27 Mar, *BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.18421"><u>https://arxiv.org/abs/2403.18421</u></a>

- 
27 Mar, *ViTAR: Vision Transformer with Any Resolution*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.18361"><u>https://arxiv.org/abs/2403.18361</u></a>

- 
27 Mar, *Long-form Factuality in Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.18802"><u>https://arxiv.org/abs/2403.18802</u></a>

- 
27 Mar, *Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.18814"><u>https://arxiv.org/abs/2403.18814</u></a>

- 
26 Mar, *LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.17919"><u>https://arxiv.org/abs/2403.17919</u></a>

- 
26 Mar, *Mechanistic Design and Scaling of Hybrid Architectures*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.17844"><u>https://arxiv.org/abs/2403.17844</u></a>

- 
28 Mar, *MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.19651"><u>https://arxiv.org/abs/2403.19651</u></a>

- 
28 Mar, *Model Stock: All We Need Is Just a Few Fine-Tuned Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2403.19522"><u>https://arxiv.org/abs/2403.19522</u></a>



## **April 2024**

- 
1 Apr, *Do Language Models Plan Ahead for Future Tokens?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.00859"><u>https://arxiv.org/abs/2404.00859</u></a>

- 
1 Apr, *Bigger is not Always Better: Scaling Properties of Latent Diffusion Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.01367"><u>https://arxiv.org/abs/2404.01367</u></a>

- 
1 Apr, *The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.01204"><u>https://arxiv.org/abs/2404.01204</u></a>

- 
1 Apr, *Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.04478"><u>https://arxiv.org/abs/2404.04478</u></a>

- 
2 Apr, *Mixture-of-Depths: Dynamically Allocating Compute in Transformer-Based Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.02258"><u>https://arxiv.org/abs/2404.02258</u></a>

- 
2 Apr, *Long-context LLMs Struggle with Long In-context Learning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.02060"><u>https://arxiv.org/abs/2404.02060</u></a>

- 
2 Apr, *Emergent Abilities in Reduced-Scale Generative Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.02204"><u>https://arxiv.org/abs/2404.02204</u></a>

- 
2 Apr, *Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.02151"><u>https://arxiv.org/abs/2404.02151</u></a>

- 
3 Apr, *On the Scalability of Diffusion-based Text-to-Image Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.02883"><u>https://arxiv.org/abs/2404.02883</u></a>

- 
3 Apr, *BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.02827"><u>https://arxiv.org/abs/2404.02827</u></a>

- 
3 Apr, *Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.02747"><u>https://arxiv.org/abs/2404.02747</u></a>

- 
4 Apr, *Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.02151"><u>https://arxiv.org/abs/2404.02151</u></a>

- 
4 Apr, *Training LLMs over Neurally Compressed Text*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.03626"><u>https://arxiv.org/abs/2404.03626</u></a>

- 
4 Apr, *CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.03820"><u>https://arxiv.org/abs/2404.03820</u></a>

- 
5 Apr, *ReFT: Representation Finetuning for Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.03592"><u>https://arxiv.org/abs/2404.03592</u></a>

- 
5 Apr, *Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.03862"><u>https://arxiv.org/abs/2404.03862</u></a>

- 
5 Apr, *Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.04256"><u>https://arxiv.org/abs/2404.04256</u></a>

- 
8 Apr, *AutoCodeRover: Autonomous Program Improvement*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.05427"><u>https://arxiv.org/abs/2404.05427</u></a>

- 
8 Apr, *Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.05892"><u>https://arxiv.org/abs/2404.05892</u></a>

- 
8 Apr, *CodecLM: Aligning Language Models with Tailored Synthetic Data*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.05875"><u>https://arxiv.org/abs/2404.05875</u></a>

- 
9 Apr, *MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.06395"><u>https://arxiv.org/abs/2404.06395</u></a>

- 
9 Apr, *Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.06209"><u>https://arxiv.org/abs/2404.06209</u></a>

- 
9 Apr, *LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.05961"><u>https://arxiv.org/abs/2404.05961</u></a>

- 
10 Apr, *Adapting LLaMA Decoder to Vision Transformer*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.06773"><u>https://arxiv.org/abs/2404.06773</u></a>

- 
10 Apr, *Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.07143"><u>https://arxiv.org/abs/2404.07143</u></a>

- 
11 Apr, *LLoCO: Learning Long Contexts Offline*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.07979"><u>https://arxiv.org/abs/2404.07979</u></a>

- 
11 Apr, *JetMoE: Reaching Llama2 Performance with 0.1M Dollars*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.07413"><u>https://arxiv.org/abs/2404.07413</u></a>

- 
11 Apr, *Best Practices and Lessons Learned on Synthetic Data for Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.07503"><u>https://arxiv.org/abs/2404.07503</u></a>

- 
11 Apr, *Rho-1: Not All Tokens Are What You Need*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.07965"><u>https://arxiv.org/abs/2404.07965</u></a>

- 
12 Apr, *Pre-training Small Base LMs with Fewer Tokens*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.08634"><u>https://arxiv.org/abs/2404.08634</u></a>

- 
12 Apr, *Dataset Reset Policy Optimization for RLHF*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.08495"><u>https://arxiv.org/abs/2404.08495</u></a>

- 
13 Apr, *LLM In-Context Recall is Prompt Dependent*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.08865"><u>https://arxiv.org/abs/2404.08865</u></a>

- 
15 Apr, *State Space Model for New-Generation Network Alternative to Transformers: A Survey*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.09516"><u>https://arxiv.org/abs/2404.09516</u></a>

- 
15 Apr, *Chinchilla Scaling: A Replication Attempt*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.10102"><u>https://arxiv.org/abs/2404.10102</u></a>

- 
15 Apr, *Learn Your Reference Model for Real Good Alignment*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.09656"><u>https://arxiv.org/abs/2404.09656</u></a>

- 
16 Apr, *Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.10719"><u>https://arxiv.org/abs/2404.10719</u></a>

- 
16 Apr, *Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.08197"><u>https://arxiv.org/abs/2404.08197</u></a>

- 
16 Apr, *How Faithful Are RAG Models? Quantifying the Tug-of-War Between RAG and LLMs' Internal Prior*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.10198"><u>https://arxiv.org/abs/2404.10198</u></a>

- 
17 Apr, *A Survey on Retrieval-Augmented Text Generation for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.10981"><u>https://arxiv.org/abs/2404.10981</u></a>

- 
18 Apr, *When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.12365"><u>https://arxiv.org/abs/2404.12365</u></a>

- 
18 Apr, *Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.12253"><u>https://arxiv.org/abs/2404.12253</u></a>

- 
18 Apr, *OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.12195"><u>https://arxiv.org/abs/2404.12195</u></a>

- 
19 Apr, *The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.13208"><u>https://arxiv.org/abs/2404.13208</u></a>

- 
22 Apr, *How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.14047"><u>https://arxiv.org/abs/2404.14047</u></a>

- 
22 Apr, *Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.14219"><u>https://arxiv.org/abs/2404.14219</u></a>

- 
22 Apr, *OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.14619"><u>https://arxiv.org/abs/2404.14619</u></a>

- 
22 Apr, *A Survey on Self-Evolution of Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.14662"><u>https://arxiv.org/abs/2404.14662</u></a>

- 
23 Apr, *Multi-Head Mixture-of-Experts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.15045"><u>https://arxiv.org/abs/2404.15045</u></a>

- 
23 Apr, *NExT: Teaching Large Language Models to Reason about Code Execution*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.14662"><u>https://arxiv.org/abs/2404.14662</u></a>

- 
23 Apr, *Graph Machine Learning in the Era of Large Language Models (LLMs)*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.14928"><u>https://arxiv.org/abs/2404.14928</u></a>

- 
24 Apr, *Retrieval Head Mechanistically Explains Long-Context Factuality*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.15574"><u>https://arxiv.org/abs/2404.15574</u></a>

- 
25 Apr, *Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.16710"><u>https://arxiv.org/abs/2404.16710</u></a>

- 
25 Apr, *Make Your LLM Fully Utilize the Context*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.16811"><u>https://arxiv.org/abs/2404.16811</u></a>

- 
28 Apr, *LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.00732"><u>https://arxiv.org/abs/2405.00732</u></a>

- 
30 Apr, *Better &amp; Faster Large Language Models via Multi-token Prediction*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.19737"><u>https://arxiv.org/abs/2404.19737</u></a>

- 
30 Apr, *RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.19543"><u>https://arxiv.org/abs/2404.19543</u></a>

- 
30 Apr, *A Primer on the Inner Workings of Transformer-based Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.00208"><u>https://arxiv.org/abs/2405.00208</u></a>

- 
30 Apr, *When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.19705"><u>https://arxiv.org/abs/2404.19705</u></a>

- 
30 Apr, *KAN: Kolmogorov–Arnold Networks*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2404.19756"><u>https://arxiv.org/abs/2404.19756</u></a>



## **May 2024**

- 
1 May, *Is Bigger Edit Batch Size Always Better? An Empirical Study on Model Editing with Llama-3*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.00664"><u>https://arxiv.org/abs/2405.00664</u></a>

- 
1 May, *Self-Play Preference Optimization for Language Model Alignment*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.00675"><u>https://arxiv.org/abs/2405.00675</u></a>

- 
1 May, *A Careful Examination of Large Language Model Performance on Grade School Arithmetic*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.00332"><u>https://arxiv.org/abs/2405.00332</u></a>

- 
2 May, *Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.01535"><u>https://arxiv.org/abs/2405.01535</u></a>

- 
3 May, *What Matters When Building Vision-Language Models?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.02246"><u>https://arxiv.org/abs/2405.02246</u></a>

- 
5 May, *Is Flash Attention Stable?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.02803"><u>https://arxiv.org/abs/2405.02803</u></a>

- 
7 May, *vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.04437"><u>https://arxiv.org/abs/2405.04437</u></a>

- 
7 May, *xLSTM: Extended Long Short-Term Memory*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.04517"><u>https://arxiv.org/abs/2405.04517</u></a>

- 
8 May, *You Only Cache Once: Decoder-Decoder Architectures for Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.05254"><u>https://arxiv.org/abs/2405.05254</u></a>

- 
8 May, *DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.04434"><u>https://arxiv.org/abs/2405.04434</u></a>

- 
8 May, *Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.05417"><u>https://arxiv.org/abs/2405.05417</u></a>

- 
9 May, *Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.05904"><u>https://arxiv.org/abs/2405.05904</u></a>

- 
10 May, *Value Augmented Sampling for Language Model Alignment and Personalization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.06639"><u>https://arxiv.org/abs/2405.06639</u></a>

- 
12 May, *PHUDGE: Phi-3 as Scalable Judge*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.08029"><u>https://arxiv.org/abs/2405.08029</u></a>

- 
13 May, *RLHF Workflow: From Reward Modeling to Online RLHF*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.07863"><u>https://arxiv.org/abs/2405.07863</u></a>

- 
15 May, *LoRA Learns Less and Forgets Less*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.09673"><u>https://arxiv.org/abs/2405.09673</u></a>

- 
15 May, *Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.09215"><u>https://arxiv.org/abs/2405.09215</u></a>

- 
16 May, *Chameleon: Mixed-Modal Early-Fusion Foundation Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.09818"><u>https://arxiv.org/abs/2405.09818</u></a>

- 
17 May, *Towards Modular LLMs by Building and Reusing a Library of LoRAs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.11157"><u>https://arxiv.org/abs/2405.11157</u></a>

- 
19 May, *SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.11582"><u>https://arxiv.org/abs/2405.11582</u></a>

- 
20 May, *MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.12130"><u>https://arxiv.org/abs/2405.12130</u></a>

- 
22 May, *Attention as an RNN*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.13956"><u>https://arxiv.org/abs/2405.13956</u></a>

- 
22 May, *Dense Connector for MLLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.13800"><u>https://arxiv.org/abs/2405.13800</u></a>

- 
23 May, *AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.14129"><u>https://arxiv.org/abs/2405.14129</u></a>

- 
23 May, *SimPO: Simple Preference Optimization with a Reference-Free Reward*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.14734"><u>https://arxiv.org/abs/2405.14734</u></a>

- 
23 May, *Instruction Tuning With Loss Over Instructions*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.14394"><u>https://arxiv.org/abs/2405.14394</u></a>

- 
24 May, *The Road Less Scheduled*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.15682"><u>https://arxiv.org/abs/2405.15682</u></a>

- 
26 May, *Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.15319"><u>https://arxiv.org/abs/2405.15319</u></a>

- 
26 May, *gzip Predicts Data-dependent Scaling Laws*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.16684"><u>https://arxiv.org/abs/2405.16684</u></a>

- 
27 May, *Trans-LoRA: Towards Data-free Transferable Parameter Efficient Finetuning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.17258"><u>https://arxiv.org/abs/2405.17258</u></a>

- 
28 May, *VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.17991"><u>https://arxiv.org/abs/2405.17991</u></a>

- 
28 May, *LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.18377"><u>https://arxiv.org/abs/2405.18377</u></a>

- 
29 May, *Contextual Position Encoding: Learning to Count What's Important*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2405.18719"><u>https://arxiv.org/abs/2405.18719</u></a>



## **June 2024**

- 
2 Jun, *Show, Don't Tell: Aligning Language Models with Demonstrated Feedback*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.00888"><u>https://arxiv.org/abs/2406.00888</u></a>

- 
3 Jun, *Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.06563"><u>https://arxiv.org/abs/2406.06563</u></a>

- 
3 Jun, *OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.01775"><u>https://arxiv.org/abs/2406.01775</u></a>

- 
3 Jun, *The Geometry of Categorical and Hierarchical Concepts in Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.01506"><u>https://arxiv.org/abs/2406.01506</u></a>

- 
3 Jun, *Towards Scalable Automated Alignment of LLMs: A Survey*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.01252"><u>https://arxiv.org/abs/2406.01252</u></a>

- 
4 Jun, *Scalable MatMul-free Language Modeling*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.02528"><u>https://arxiv.org/abs/2406.02528</u></a>

- 
4 Jun, *Block Transformer: Global-to-Local Language Modeling for Fast Inference*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.02657"><u>https://arxiv.org/abs/2406.02657</u></a>

- 
6 Jun, *Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04271"><u>https://arxiv.org/abs/2406.04271</u></a>

- 
6 Jun, *The Prompt Report: A Systematic Survey of Prompting Techniques*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.06608"><u>https://arxiv.org/abs/2406.06608</u></a>

- 
6 Jun, *Transformers Need Glasses! Information Over-Squashing in Language Tasks*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04267"><u>https://arxiv.org/abs/2406.04267</u></a>

- 
6 Jun, *Are We Done with MMLU?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04127"><u>https://arxiv.org/abs/2406.04127</u></a>

- 
6 Jun, *Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04314"><u>https://arxiv.org/abs/2406.04314</u></a>

- 
7 Jun, *Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04594"><u>https://arxiv.org/abs/2406.04594</u></a>

- 
7 Jun, *CRAG -- Comprehensive RAG Benchmark*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04744"><u>https://arxiv.org/abs/2406.04744</u></a>

- 
7 Jun, *WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04770"><u>https://arxiv.org/abs/2406.04770</u></a>

- 
7 Jun, *Mixture-of-Agents Enhances Large Language Model Capabilities*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04692"><u>https://arxiv.org/abs/2406.04692</u></a>

- 
7 Jun, *BERTs are Generative In-Context Learners*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.04823"><u>https://arxiv.org/abs/2406.04823</u></a>

- 
7 Jun, *3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.05132"><u>https://arxiv.org/abs/2406.05132</u></a>

- 
8 Jun, *Creativity Has Left the Chat: The Price of Debiasing Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.05587"><u>https://arxiv.org/abs/2406.05587</u></a>

- 
10 Jun, *Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.06525"><u>https://arxiv.org/abs/2406.06525</u></a>

- 
10 Jun, *Margin-aware Preference Optimization for Aligning Diffusion Models Without Reference*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.06424"><u>https://arxiv.org/abs/2406.06424</u></a>

- 
10 Jun, *Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.06469"><u>https://arxiv.org/abs/2406.06469</u></a>

- 
10 Jun, *Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.05955"><u>https://arxiv.org/abs/2406.05955</u></a>

- 
10 Jun, *Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.06326"><u>https://arxiv.org/abs/2406.06326</u></a>

- 
11 Jun, *An Image is Worth 32 Tokens for Reconstruction and Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.07550"><u>https://arxiv.org/abs/2406.07550</u></a>

- 
11 Jun, *TextGrad: Automatic "Differentiation" via Text*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.07496"><u>https://arxiv.org/abs/2406.07496</u></a>

- 
11 Jun, *Simple and Effective Masked Diffusion Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.07524"><u>https://arxiv.org/abs/2406.07524</u></a>

- 
11 Jun, *Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.07138"><u>https://arxiv.org/abs/2406.07138</u></a>

- 
11 Jun, *Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.07522"><u>https://arxiv.org/abs/2406.07522</u></a>

- 
12 Jun, *Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.08464"><u>https://arxiv.org/abs/2406.08464</u></a>

- 
12 Jun, *What If We Recaption Billions of Web Images with LLaMA-3?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.08478"><u>https://arxiv.org/abs/2406.08478</u></a>

- 
12 Jun, *Large Language Model Unlearning via Embedding-Corrupted Prompts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.07933"><u>https://arxiv.org/abs/2406.07933</u></a>

- 
12 Jun, *Large Language Models Must Be Taught to Know What They Don't Know*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.08391"><u>https://arxiv.org/abs/2406.08391</u></a>

- 
12 Jun, *An Empirical Study of Mamba-based Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.07887"><u>https://arxiv.org/abs/2406.07887</u></a>

- 
12 Jun, *Discovering Preference Optimization Algorithms with and for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.08414"><u>https://arxiv.org/abs/2406.08414</u></a>

- 
13 Jun, *Transformers Meet Neural Algorithmic Reasoners*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.09308"><u>https://arxiv.org/abs/2406.09308</u></a>

- 
13 Jun, *MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.09297"><u>https://arxiv.org/abs/2406.09297</u></a>

- 
13 Jun, *An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.09415"><u>https://arxiv.org/abs/2406.09415</u></a>

- 
13 Jun, *FouRA: Fourier Low Rank Adaptation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.08798"><u>https://arxiv.org/abs/2406.08798</u></a>

- 
14 Jun, *Bootstrapping Language Models with DPO Implicit Rewards*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.09760"><u>https://arxiv.org/abs/2406.09760</u></a>

- 
14 Jun, *Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.10209"><u>https://arxiv.org/abs/2406.10209</u></a>

- 
14 Jun, *Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.10216"><u>https://arxiv.org/abs/2406.10216</u></a>

- 
16 Jun, *THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.10996"><u>https://arxiv.org/abs/2406.10996</u></a>

- 
17 Jun, *Task Me Anything*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11775"><u>https://arxiv.org/abs/2406.11775</u></a>

- 
17 Jun, *How Do Large Language Models Acquire Factual Knowledge During Pretraining?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11813"><u>https://arxiv.org/abs/2406.11813</u></a>

- 
17 Jun, *mDPO: Conditional Preference Optimization for Multimodal Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11839"><u>https://arxiv.org/abs/2406.11839</u></a>

- 
17 Jun, *Nemotron-4 340B Technical Report*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11704"><u>https://arxiv.org/abs/2406.11704</u></a>

- 
17 Jun, *DataComp-LM: In Search of the Next Generation of Training Sets for Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11794"><u>https://arxiv.org/abs/2406.11794</u></a>

- 
17 Jun, *Tokenization Falling Short: The Curse of Tokenization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11687"><u>https://arxiv.org/abs/2406.11687</u></a>

- 
17 Jun, *DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11931"><u>https://arxiv.org/abs/2406.11931</u></a>

- 
17 Jun, *Unveiling Encoder-Free Vision-Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11832"><u>https://arxiv.org/abs/2406.11832</u></a>

- 
17 Jun, *Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11817"><u>https://arxiv.org/abs/2406.11817</u></a>

- 
17 Jun, *HARE: HumAn pRiors, a key to small language model Efficiency*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11410"><u>https://arxiv.org/abs/2406.11410</u></a>

- 
17 Jun, *Measuring memorization in RLHF for code completion*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.11715"><u>https://arxiv.org/abs/2406.11715</u></a>

- 
17 Jun, *Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.12034"><u>https://arxiv.org/abs/2406.12034</u></a>

- 
18 Jun, *From RAGs to Rich Parameters: Probing How Language Models Utilize External Knowledge Over Parametric Information for Factual Queries*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.12824"><u>https://arxiv.org/abs/2406.12824</u></a>

- 
18 Jun, *Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.12624"><u>https://arxiv.org/abs/2406.12624</u></a>

- 
19 Jun, *Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.13121"><u>https://arxiv.org/abs/2406.13121</u></a>

- 
20 Jun, *Instruction Pre-Training: Language Models are Supervised Multitask Learners*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.14491"><u>https://arxiv.org/abs/2406.14491</u></a>

- 
20 Jun, *Can LLMs Learn by Teaching? A Preliminary Study*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.14629"><u>https://arxiv.org/abs/2406.14629</u></a>

- 
21 Jun, *A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.14972"><u>https://arxiv.org/abs/2406.14972</u></a>

- 
21 Jun, *LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.15319"><u>https://arxiv.org/abs/2406.15319</u></a>

- 
21 Jun, *MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.14909"><u>https://arxiv.org/abs/2406.14909</u></a>

- 
21 Jun, *Efficient Continual Pre-training by Mitigating the Stability Gap*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.14833"><u>https://arxiv.org/abs/2406.14833</u></a>

- 
24 Jun, *Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.16747"><u>https://arxiv.org/abs/2406.16747</u></a>

- 
24 Jun, *WARP: On the Benefits of Weight Averaged Rewarded Policies*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.16768"><u>https://arxiv.org/abs/2406.16768</u></a>

- 
24 Jun, *Adam-mini: Use Fewer Learning Rates To Gain More*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.16793"><u>https://arxiv.org/abs/2406.16793</u></a>

- 
25 Jun, *The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.17557"><u>https://arxiv.org/abs/2406.17557</u></a>

- 
25 Jun, *LongIns: A Challenging Long-context Instruction-based Exam for LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.17588"><u>https://arxiv.org/abs/2406.17588</u></a>

- 
25 Jun, *Following Length Constraints in Instructions*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.17744"><u>https://arxiv.org/abs/2406.17744</u></a>

- 
26 Jun, *A Closer Look into Mixture-of-Experts in Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.18219"><u>https://arxiv.org/abs/2406.18219</u></a>

- 
26 Jun, *RouteLLM: Learning to Route LLMs with Preference Data*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.18665"><u>https://arxiv.org/abs/2406.18665</u></a>

- 
26 Jun, *Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.18629"><u>https://arxiv.org/abs/2406.18629</u></a>

- 
27 Jun, *Dataset Size Recovery from LoRA Weights*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.19395"><u>https://arxiv.org/abs/2406.19395</u></a>

- 
27 Jun, *From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.19292"><u>https://arxiv.org/abs/2406.19292</u></a>

- 
27 Jun, *Changing Answer Order Can Decrease MMLU Accuracy*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.19470"><u>https://arxiv.org/abs/2406.19470</u></a>

- 
28 Jun, *Direct Preference Knowledge Distillation for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.19774"><u>https://arxiv.org/abs/2406.19774</u></a>

- 
28 Jun, *LLM Critics Help Catch LLM Bugs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.00215"><u>https://arxiv.org/abs/2407.00215</u></a>

- 
28 Jun, *Scaling Synthetic Data Creation with 1,000,000,000 Personas*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2406.20094"><u>https://arxiv.org/abs/2406.20094</u></a>



## **Jul 2024**

- 
1 Jul, *LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.01490"><u>https://arxiv.org/abs/2407.01490</u></a>

- 
1 Jul, *Searching for Best Practices in Retrieval-Augmented Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.01219"><u>https://arxiv.org/abs/2407.01219</u></a>

- 
1 Jul, *Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.01906"><u>https://arxiv.org/abs/2407.01906</u></a>

- 
1 Jul, *Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.01392"><u>https://arxiv.org/abs/2407.01392</u></a>

- 
1 Jul, *Eliminating Position Bias of Language Models: A Mechanistic Approach*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.01100"><u>https://arxiv.org/abs/2407.01100</u></a>

- 
2 Jul, *JMInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.02490"><u>https://arxiv.org/abs/2407.02490</u></a>

- 
2 Jul, *TokenPacker: Efficient Visual Projector for Multimodal LLM*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.02392"><u>https://arxiv.org/abs/2407.02392</u></a>

- 
2 Jul, *Reasoning in Large Language Models: A Geometric Perspective*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.02678"><u>https://arxiv.org/abs/2407.02678</u></a>

- 
2 Jul, *RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.02485"><u>https://arxiv.org/abs/2407.02485</u></a>

- 
3 Jul, *AgentInstruct: Toward Generative Teaching with Agentic Flows*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.03502"><u>https://arxiv.org/abs/2407.03502</u></a>

- 
3 Jul, *HEMM: Holistic Evaluation of Multimodal Foundation Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.03418"><u>https://arxiv.org/abs/2407.03418</u></a>

- 
4 Jul, *Mixture of A Million Experts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.04153"><u>https://arxiv.org/abs/2407.04153</u></a>

- 
5 Jul, *Learning to (Learn at Test Time): RNNs with Expressive Hidden States*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.04620"><u>https://arxiv.org/abs/2407.04620</u></a>

- 
9 Jul, *Vision Language Models Are Blind*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.06581"><u>https://arxiv.org/abs/2407.06581</u></a>

- 
9 Jul, *Self-Recognition in Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.06946"><u>https://arxiv.org/abs/2407.06946</u></a>

- 
10 Jul, *Inference Performance Optimization for Large Language Models on CPUs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.07304"><u>https://arxiv.org/abs/2407.07304</u></a>

- 
11 Jul, *Gradient Boosting Reinforcement Learning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.08250"><u>https://arxiv.org/abs/2407.08250</u></a>

- 
11 Jul, *FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.08608"><u>https://arxiv.org/abs/2407.08608</u></a>

- 
12 Jul, *SpreadsheetLLM: Encoding Spreadsheets for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.09025"><u>https://arxiv.org/abs/2407.09025</u></a>

- 
12 Jul, *New Desiderata for Direct Preference Optimization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.09072"><u>https://arxiv.org/abs/2407.09072</u></a>

- 
12 Jul, *Context Embeddings for Efficient Answer Generation in RAG*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.09252"><u>https://arxiv.org/abs/2407.09252</u></a>

- 
15 Jul, *Qwen2 Technical Report*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.10671"><u>https://arxiv.org/abs/2407.10671</u></a>

- 
15 Jul, *The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.10457"><u>https://arxiv.org/abs/2407.10457</u></a>

- 
15 Jul, *From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.11239"><u>https://arxiv.org/abs/2407.11239</u></a>

- 
16 Jul, *GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.12077"><u>https://arxiv.org/abs/2407.12077</u></a>

- 
16 Jul, *Scaling Diffusion Transformers to 16 Billion Parameters*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.11633"><u>https://arxiv.org/abs/2407.11633</u></a>

- 
16 Jul, *NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.11963"><u>https://arxiv.org/abs/2407.11963</u></a>

- 
17 Jul, *Patch-Level Training for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.12665"><u>https://arxiv.org/abs/2407.12665</u></a>

- 
17 Jul, *LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.12772"><u>https://arxiv.org/abs/2407.12772</u></a>

- 
17 Jul, *A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.12994"><u>https://arxiv.org/abs/2407.12994</u></a>

- 
17 Jul, *Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.12327"><u>https://arxiv.org/abs/2407.12327</u></a>

- 
18 Jul, *Attention Overflow: Language Model Input Blur during Long-Context Missing Items Recommendation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.13481"><u>https://arxiv.org/abs/2407.13481</u></a>

- 
18 Jul, *Weak-to-Strong Reasoning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.13647"><u>https://arxiv.org/abs/2407.13647</u></a>

- 
18 Jul, *Understanding Reference Policies in Direct Preference Optimization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.13709"><u>https://arxiv.org/abs/2407.13709</u></a>

- 
18 Jul, *Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.13623"><u>https://arxiv.org/abs/2407.13623</u></a>

- 
19 Jul, *BOND: Aligning LLMs with Best-of-N Distillation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.14622"><u>https://arxiv.org/abs/2407.14622</u></a>

- 
19 Jul, *Compact Language Models via Pruning and Knowledge Distillation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.14679"><u>https://arxiv.org/abs/2407.14679</u></a>

- 
19 Jul, *LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.14057"><u>https://arxiv.org/abs/2407.14057</u></a>

- 
22 Jul, *Mini-Sequence Transformer: Optimizing Intermediate Memory for Long Sequences Training*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.15892"><u>https://arxiv.org/abs/2407.15892</u></a>

- 
22 Jul, *DDK: Distilling Domain Knowledge for Efficient Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.16154"><u>https://arxiv.org/abs/2407.16154</u></a>

- 
23 Jul, *Generation Constraint Scaling Can Mitigate Hallucination*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.16908"><u>https://arxiv.org/abs/2407.16908</u></a>

- 
23 Jul, *Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.16833"><u>https://arxiv.org/abs/2407.16833</u></a>

- 
23 Jul, *Course-Correction: Safety Alignment Using Synthetic Preferences*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.16637"><u>https://arxiv.org/abs/2407.16637</u></a>

- 
26 Jul, *Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.16607"><u>https://arxiv.org/abs/2407.16607</u></a>

- 
28 Jul, *Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.19594"><u>https://arxiv.org/abs/2407.19594</u></a>

- 
29 Jul, *Improving Retrieval Augmented Language Model with Self-Reasoning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.19813"><u>https://arxiv.org/abs/2407.19813</u></a>

- 
29 Jul, *Apple Intelligence Foundation Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.21075"><u>https://arxiv.org/abs/2407.21075</u></a>

- 
30 Jul, *ThinK: Thinner Key Cache by Query-Driven Pruning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.21018"><u>https://arxiv.org/abs/2407.21018</u></a>

- 
31 Jul, *The Llama 3 Herd of Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2407.21783"><u>https://arxiv.org/abs/2407.21783</u></a>

- 
31 Jul, *Gemma 2: Improving Open Language Models at a Practical Size*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.00118"><u>https://arxiv.org/abs/2408.00118</u></a>



## **August 2024**

- 
1 Aug, S*AM 2: Segment Anything in Images and Videos,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.00714"><u>https://arxiv.org/abs/2408.00714</u></a>

- 
2 Aug, *POA: Pre-training Once for Models of All Sizes,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.01031"><u>https://arxiv.org/abs/2408.01031</u></a>

- 
2 Aug, *RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.01262"><u>https://arxiv.org/abs/2408.01262</u></a>

- 
2 Aug, *A Survey of Mamba,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.01129"><u>https://arxiv.org/abs/2408.01129</u></a>

- 
3 Aug, *MiniCPM-V: A GPT-4V Level MLLM on Your Phone,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.01800"><u>https://arxiv.org/abs/2408.01800</u></a>

- 
5 Aug, *RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.02545"><u>https://arxiv.org/abs/2408.02545</u></a>

- 
5 Aug, *Self-Taught Evaluators,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.02666"><u>https://arxiv.org/abs/2408.02666</u></a>

- 
5 Aug, *BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.02600"><u>https://arxiv.org/abs/2408.02600</u></a>

- 
5 Aug, *Self-Taught Evaluators,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.02666"><u>https://arxiv.org/abs/2408.02666</u></a>

- 
7 Aug, *EXAONE 3.0 7.8B Instruction Tuned Language Model,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.03541"><u>https://arxiv.org/abs/2408.03541</u></a>

- 
7 Aug, *1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.03506"><u>https://arxiv.org/abs/2408.03506</u></a>

- 
8 Aug, *Conversational Prompt Engineering,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.04560"><u>https://arxiv.org/abs/2408.04560</u></a>

- 
8 Aug, *Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.04303"><u>https://arxiv.org/abs/2408.04303</u></a>

- 
12 Aug, *The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.06292"><u>https://arxiv.org/abs/2408.06292</u></a>

- 
15 Aug, *Hermes 3 Technical Report,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.12570"><u>https://arxiv.org/abs/2408.12570</u></a>

- 
19 Aug, *Customizing Language Models with Instance-wise LoRA for Sequential Recommendation,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.10159"><u>https://arxiv.org/abs/2408.10159</u></a>

- 
20 Aug*, Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.10615"><u>https://arxiv.org/abs/2408.10615</u></a>

- 
20 Aug, *To Code, or Not To Code? Exploring Impact of Code in Pre-training,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.10914"><u>https://arxiv.org/abs/2408.10914</u></a>

- 
21 Aug ,* LLM Pruning and Distillation in Practice: The Minitron Approach, *<a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.11796"><u>https://arxiv.org/abs/2408.11796</u></a>

- 
22 Aug, *Jamba-1.5: Hybrid Transformer-Mamba Models at Scale,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.12570"><u>https://arxiv.org/abs/2408.12570</u></a>

- 
22 Aug, *Controllable Text Generation for Large Language Models: A Survey,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.12599"><u>https://arxiv.org/abs/2408.12599</u></a>

- 
23 Aug, *Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.13233"><u>https://arxiv.org/abs/2408.13233</u></a>

- 
26 Aug, *A Practitioner's Guide to Continual Multimodal Pretraining,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.14471"><u>https://arxiv.org/abs/2408.14471</u></a>

- 
26 Aug, *Building and better understanding vision-language models: insights and future directions,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.12637"><u>https://arxiv.org/abs/2408.12637</u></a>

- 
26 Aug, *CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.14572"><u>https://arxiv.org/abs/2408.14572</u></a>

- 
27 Aug, *The Mamba in the Llama: Distilling and Accelerating Hybrid Models,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.15237"><u>https://arxiv.org/abs/2408.15237</u></a>

- 
28 Aug, *ReMamba: Equip Mamba with Effective Long-Sequence Modeling,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.15496"><u>https://arxiv.org/abs/2408.15496</u></a>

- 
29 Aug, *Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2408.16737"><u>https://arxiv.org/abs/2408.16737</u></a>

- 
31 Aug, *LongRecipe: Recipe for Efficient Long Context Generalization in Large Languge Models,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.00509"><u>https://arxiv.org/abs/2409.00509</u></a>



## **September 2024**

- 
3 Sep, *OLMoE: Open Mixture-of-Experts Language Models,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.02060"><u>https://arxiv.org/abs/2409.02060</u></a>

- 
3 Sep 2024, *In Defense of RAG in the Era of Long-Context Language Models,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.01666"><u>https://arxiv.org/abs/2409.01666</u></a>

- 
5 Sep, *Attention Heads of Large Language Models: A Survey,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.03752"><u>https://arxiv.org/abs/2409.03752</u></a>

- 
5 Sep, *LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.02897"><u>https://arxiv.org/abs/2409.02897</u></a>

- 
5 Sep, *How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.03810"><u>https://arxiv.org/abs/2409.03810</u></a>

- 
6 Sep, T*heory, Analysis, and Best Practices for Sigmoid Self-Attention,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.04431"><u>https://arxiv.org/abs/2409.04431</u></a>

- 
10 Sep, *LLaMA-Omni: Seamless Speech Interaction with Large Language Models,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.06666"><u>https://arxiv.org/abs/2409.06666</u></a>

- 
10 Sep, *What is the Role of Small Models in the LLM Era: A Survey,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.06857"><u>https://arxiv.org/abs/2409.06857</u></a>

- 
11 Sep, *Policy Filtration in RLHF to Fine-Tune LLM for Code Generation,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.06957"><u>https://arxiv.org/abs/2409.06957</u></a>

- 
16 Sep, *RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.10516"><u>https://arxiv.org/abs/2409.10516</u></a>

- 
18 Sep, *Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.12122"><u>https://arxiv.org/abs/2409.12122</u></a>

- 
18 Sep, *Qwen2.5-Coder Technical Report*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.12186"><u>https://arxiv.org/abs/2409.12186</u></a>

- 
21 Sep, *Instruction Following without Instruction Tuning,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.14254"><u>https://arxiv.org/abs/2409.14254</u></a>

- 
30 Sep, I*s Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.20059"><u>https://arxiv.org/abs/2409.20059</u></a>

- 
30 Sep, *The Perfect Blend: Redefining RLHF with Mixture of Judges,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2409.20370"><u>https://arxiv.org/abs/2409.20370</u></a> (New paper by Meta on how they did RLHF for Llama 3)



## **October 2024**

- 
1 Oct, *Addition is All You Need for Energy-efficient Language Models,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.00907"><u>https://arxiv.org/abs/2410.00907</u></a>

- 
2 Oct *Quantifying Generalization Complexity for Large Language Models,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.01769"><u>https://arxiv.org/abs/2410.01769</u></a>

- 
2 Oct, *When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.01792"><u>https://arxiv.org/abs/2410.01792</u></a>

- 
2 Oct, W*ere RNNs All We Needed?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.01201"><u>https://arxiv.org/abs/2410.01201</u></a>

- 
3 Oct, *Selective Attention Improves Transformer*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.02703"><u>https://arxiv.org/abs/2410.02703</u></a>

- 
3 Oct, *LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.02707"><u>https://arxiv.org/abs/2410.02707</u></a>

- 
3 Oct, *LLaVA-Critic: Learning to Evaluate Multimodal Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.02712"><u>https://arxiv.org/abs/2410.02712</u></a>

- 
7 Oct, *Differential Transformer*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.05258"><u>https://arxiv.org/abs/2410.05258</u></a>

- 
7 Oct, *GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.05229"><u>https://arxiv.org/abs/2410.05229</u></a>

- 
8 Oct, *ARIA: An Open Multimodal Native Mixture-of-Experts Model*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.05993"><u>https://arxiv.org/abs/2410.05993</u></a>

- 
8 Oct, *O1 Replication Journey: A Strategic Progress Report -- Part 1*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.18982"><u>https://arxiv.org/abs/2410.18982</u></a>

- 
8 Oct, *Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG,* <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.05983"><u>https://arxiv.org/abs/2410.05983</u></a>

- 
9 Oct, *From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.06456"><u>https://arxiv.org/abs/2410.06456</u></a>

- 
10 Oct, *KV Prediction for Improved Time to First Token*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.08391"><u>https://arxiv.org/abs/2410.08391</u></a>

- 
11 Oct, *Baichuan-Omni Technical Report*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.08565"><u>https://arxiv.org/abs/2410.08565</u></a>

- 
13 Oct, *MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.10139"><u>https://arxiv.org/abs/2410.10139</u></a>

- 
13 Oct, *LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.09732"><u>https://arxiv.org/abs/2410.09732</u></a>

- 
15 Oct, *AFlow: Automating Agentic Workflow Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.10762"><u>https://arxiv.org/abs/2410.10762</u></a>

- 
15 Oct, *Toward General Instruction-Following Alignment for Retrieval-Augmented Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.09584"><u>https://arxiv.org/abs/2410.09584</u></a>

- 
21 Oct, *Pre-training Distillation for Large Language Models: A Design Space Exploration*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.16215"><u>https://arxiv.org/abs/2410.16215</u></a>

- 
23 Oct, *MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.17637"><u>https://arxiv.org/abs/2410.17637</u></a>

- 
23 Oct, *Scalable Ranked Preference Optimization for Text-to-Image Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.18013"><u>https://arxiv.org/abs/2410.18013</u></a>

- 
23 Oct, *Scaling Diffusion Language Models via Adaptation from Autoregressive Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.17891"><u>https://arxiv.org/abs/2410.17891</u></a>

- 
24 Oct, *Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.19133"><u>https://arxiv.org/abs/2410.19133</u></a>

- 
25 Oct, *Counting Ability of Large Language Models and Impact of Tokenization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.19730"><u>https://arxiv.org/abs/2410.19730</u></a>

- 
25 Oct, *A Survey of Small Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.20011"><u>https://arxiv.org/abs/2410.20011</u></a>

- 
26 Oct, *Accelerating Direct Preference Optimization with Prefix Sharing*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.20305"><u>https://arxiv.org/abs/2410.20305</u></a>

- 
27 Oct, *Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.21333"><u>https://arxiv.org/abs/2410.21333</u></a>

- 
28 Oct, *LongReward: Improving Long-context Large Language Models with AI Feedback*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.21252"><u>https://arxiv.org/abs/2410.21252</u></a>

- 
28 Oct, *ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.21465"><u>https://arxiv.org/abs/2410.21465</u></a>

- 
29 Oct, *Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.21943"><u>https://arxiv.org/abs/2410.21943</u></a>

- 
30 Oct, *CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.23090"><u>https://arxiv.org/abs/2410.23090</u></a>

- 
31 Oct, *What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.23743"><u>https://arxiv.org/abs/2410.23743</u></a>

- 
31 Oct, *GPT or BERT: why not both?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.24159"><u>https://arxiv.org/abs/2410.24159</u></a>

- 
31 Oct, *Language Models can Self-Lengthen to Generate Long Texts*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2410.23933"><u>https://arxiv.org/abs/2410.23933</u></a>



## **November 2024**

- 
1 Nov, *Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.00640"><u>https://arxiv.org/abs/2411.00640</u></a>

- 
1 Nov 2024, *Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.00412"><u>https://arxiv.org/abs/2411.00412</u></a>

- 
1 Nov 2024, *Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.00492"><u>https://arxiv.org/abs/2411.00492</u></a>

- 
3 Nov, S*ample-Efficient Alignment for LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.01493"><u>https://arxiv.org/abs/2411.01493</u></a>

- 
4 Nov 2024, *A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.03350"><u>https://arxiv.org/abs/2411.03350</u></a>

- 
4 Nov, *"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.02355"><u>https://arxiv.org/abs/2411.02355</u></a>

- 
4 Nov, *Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.02462"><u>https://arxiv.org/abs/2411.02462</u></a>

- 
5 Nov, *HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.02959"><u>https://arxiv.org/abs/2411.02959</u></a>

- 
6 Nov, *Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.03823"><u>https://arxiv.org/abs/2411.03823</u></a>

- 
6 Nov, *Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.04282"><u>https://arxiv.org/abs/2411.04282</u></a>

- 
6 Nov, *Number Cookbook: Number Understanding of Language Models and How to Improve It*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.03766"><u>https://arxiv.org/abs/2411.03766</u></a>

- 
7 Nov, *Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.04996"><u>https://arxiv.org/abs/2411.04996</u></a>

- 
7 Nov, *BitNet a4.8: 4-bit Activations for 1-bit LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.04965"><u>https://arxiv.org/abs/2411.04965</u></a>

- 
7 Nov, *Scaling Laws for Precision*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.04330"><u>https://arxiv.org/abs/2411.04330</u></a>

- 
8 Nov, *Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.05966"><u>https://arxiv.org/abs/2411.05966</u></a>

- 
8 Nov, *Balancing Pipeline Parallelism with Vocabulary Parallelism*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.05288"><u>https://arxiv.org/abs/2411.05288</u></a>

- 
11 Nov, *Toward Optimal Search and Retrieval for RAG*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.07396"><u>https://arxiv.org/abs/2411.07396</u></a>

- 
12 Nov, *Large Language Models Can Self-Improve in Long-context Reasoning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.08147"><u>https://arxiv.org/abs/2411.08147</u></a>

- 
12 Nov, *Stronger Models are NOT Stronger Teachers for Instruction Tuning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.07133"><u>https://arxiv.org/abs/2411.07133</u></a>

- 
12 Nov, *Direct Preference Optimization Using Sparse Feature-Level Constraints*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.07618"><u>https://arxiv.org/abs/2411.07618</u></a>

- 
13 Nov, *Cut Your Losses in Large-Vocabulary Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.09009"><u>https://arxiv.org/abs/2411.09009</u></a>

- 
15 Nov, *Does Prompt Formatting Have Any Impact on LLM Performance?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.10541"><u>https://arxiv.org/abs/2411.10541</u></a>

- 
17 Nov, *SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.11909"><u>https://arxiv.org/abs/2411.11909</u></a>

- 
17 Nov, *SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.10958"><u>https://arxiv.org/abs/2411.10958</u></a>

- 
18 Nov, *Bi-Mamba: Towards Accurate 1-Bit State Space Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.11843"><u>https://arxiv.org/abs/2411.11843</u></a>

- 
19 Nov, RedPajama: an Open Dataset for Training Large Language Models, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.12372"><u>https://arxiv.org/abs/2411.12372</u></a>

- 
20 Nov, *Hymba: A Hybrid-head Architecture for Small Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.13676"><u>https://arxiv.org/abs/2411.13676</u></a>

- 
20 Nov, *Loss-to-Loss Prediction: Scaling Laws for All Datasets*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.12925"><u>https://arxiv.org/abs/2411.12925</u></a>

- 
21 Nov, *When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.13476"><u>https://arxiv.org/abs/2411.13476</u></a>

- 
21 Nov, *Multimodal Autoregressive Pre-training of Large Vision Encoders*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.14402"><u>https://arxiv.org/abs/2411.14402</u></a>

- 
21 Nov, *Natural Language Reinforcement Learning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.14251"><u>https://arxiv.org/abs/2411.14251</u></a>

- 
22 Nov, *Large Multi-modal Models Can Interpret Features in Large Multi-modal Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.14982"><u>https://arxiv.org/abs/2411.14982</u></a>

- 
22 Nov, *TÜLU 3: Pushing Frontiers in Open Language Model Post-Training*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.15124"><u>https://arxiv.org/abs/2411.15124</u></a>

- 
23 Nov, *MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.15296"><u>https://arxiv.org/abs/2411.15296</u></a>

- 
24 Nov, *LLMs Do Not Think Step-by-step In Implicit Reasoning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.15862"><u>https://arxiv.org/abs/2411.15862</u></a>

- 
25 Nov, *O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.16489"><u>https://arxiv.org/abs/2411.16489</u></a>

- 
26 Nov, *Star Attention: Efficient LLM Inference over Long Sequences*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.17116"><u>https://arxiv.org/abs/2411.17116</u></a>

- 
27 Nov, *Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.17691"><u>https://arxiv.org/abs/2411.17691</u></a>

- 
27 Nov, *Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.17686"><u>https://arxiv.org/abs/2411.17686</u></a>

- 
29 Nov, *Reverse Thinking Makes LLMs Stronger Reasoners*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.19865"><u>https://arxiv.org/abs/2411.19865</u></a>

- 
29 Nov, *Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2411.19943"><u>https://arxiv.org/abs/2411.19943</u></a>



## **December 2024**

- 
2 Dec, *Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.01819"><u>https://arxiv.org/abs/2412.01819</u></a>

- 
2 Dec, *X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.01824"><u>https://arxiv.org/abs/2412.01824</u></a>

- 
2 Dec, *Free Process Rewards without Process Labels*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.01981"><u>https://arxiv.org/abs/2412.01981</u></a>

- 
3 Dec, *Scaling Image Tokenizers with Grouped Spherical Quantization*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.02632"><u>https://arxiv.org/abs/2412.02632</u></a>

- 
3 Dec, *RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.02830"><u>https://arxiv.org/abs/2412.02830</u></a>

- 
4 Dec, *Perception Tokens Enhance Visual Reasoning in Multimodal Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.03548"><u>https://arxiv.org/abs/2412.03548</u></a>

- 
4 Dec, *Evaluating Language Models as Synthetic Data Generators*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.03679"><u>https://arxiv.org/abs/2412.03679</u></a>

- 
4 Dec, *Best-of-N Jailbreaking*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.03556"><u>https://arxiv.org/abs/2412.03556</u></a>

- 
4 Dec, *PaliGemma 2: A Family of Versatile VLMs for Transfer*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.03555"><u>https://arxiv.org/abs/2412.03555</u></a>

- 
5 Dec, *VisionZip: Longer is Better but Not Necessary in Vision Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.04467"><u>https://arxiv.org/abs/2412.04467</u></a>

- 
5 Dec, *Evaluating and Aligning CodeLLMs on Human Preference*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.05210"><u>https://arxiv.org/abs/2412.05210</u></a>

- 
6 Dec, *MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.05237"><u>https://arxiv.org/abs/2412.05237</u></a>

- 
6 Dec, *Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.05271"><u>https://arxiv.org/abs/2412.05271</u></a>

- 
7 Dec, *LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.05579"><u>https://arxiv.org/abs/2412.05579</u></a>

- 
8 Dec, *Does RLHF Scale? Exploring the Impacts From Data, Model, and Method*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.06000"><u>https://arxiv.org/abs/2412.06000</u></a>

- 
9 Dec, *Unraveling the Complexity of Memory in RL Agents: An Approach for Classification and Evaluation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.06531"><u>https://arxiv.org/abs/2412.06531</u></a>

- 
9 Dec, *Training Large Language Models to Reason in a Continuous Latent Space*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.06769"><u>https://arxiv.org/abs/2412.06769</u></a>

- 
9 Dec, *AutoReason: Automatic Few-Shot Reasoning Decomposition*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.06975"><u>https://arxiv.org/abs/2412.06975</u></a>

- 
11 Dec, *Large Concept Models: Language Modeling in a Sentence Representation Space*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.08821"><u>https://arxiv.org/abs/2412.08821</u></a>

- 
12 Dec, *Phi-4 Technical Report*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.08905"><u>https://arxiv.org/abs/2412.08905</u></a>

- 
13 Dec, *Byte Latent Transformer: Patches Scale Better Than Tokens*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.09871"><u>https://arxiv.org/abs/2412.09871</u></a>

- 
13 Dec, *SCBench: A KV Cache-Centric Analysis of Long-Context Methods*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.10319"><u>https://arxiv.org/abs/2412.10319</u></a>

- 
13 Dec, *Cultural Evolution of Cooperation among LLM Agents*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.10270"><u>https://arxiv.org/abs/2412.10270</u></a>

- 
13 Dec, *DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.10302"><u>https://arxiv.org/abs/2412.10302</u></a>

- 
16 Dec, *No More Adam: Learning Rate Scaling at Initialization is All You Need*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.11768"><u>https://arxiv.org/abs/2412.11768</u></a>

- 
16 Dec, *Precise Length Control in Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.11937"><u>https://arxiv.org/abs/2412.11937</u></a>

- 
16 Dec, *The Open Source Advantage in Large Language Models (LLMs)*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.12004"><u>https://arxiv.org/abs/2412.12004</u></a>

- 
16 Dec, *A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method &amp; Challenges*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.11936"><u>https://arxiv.org/abs/2412.11936</u></a>

- 
17 Dec, *Are Your LLMs Capable of Stable Reasoning?*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.13147"><u>https://arxiv.org/abs/2412.13147</u></a>

- 
18 Dec, *LLM Post-Training Recipes, Improving Reasoning in LLMs*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.14135"><u>https://arxiv.org/abs/2412.14135</u></a>

- 
18 Dec, *Hansel: Output Length Controlling Framework for Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.14033"><u>https://arxiv.org/abs/2412.14033</u></a>

- 
18 Dec, *Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.13631"><u>https://arxiv.org/abs/2412.13631</u></a>

- 
18 Dec, *Alignment Faking in Large Language Models*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.14093"><u>https://arxiv.org/abs/2412.14093</u></a>

- 
18 Dec, *SCOPE: Optimizing Key-Value Cache Compression in Long-Context Generation*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.13649"><u>https://arxiv.org/abs/2412.13649</u></a>

- 
19 Dec, *LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-Context Multitasks*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.15204"><u>https://arxiv.org/abs/2412.15204</u></a>

- 
20 Dec, *Offline Reinforcement Learning for LLM Multi-Step Reasoning*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.16145"><u>https://arxiv.org/abs/2412.16145</u></a>

- 
24 Dec, *Mulberry: Empowering MLLM with O1-like Reasoning and Reflection via Collective Monte Carlo Tree Search*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2412.18319"><u>https://arxiv.org/abs/2412.18319</u></a>

- 
31 Dec, *Titans: Learning to Memorize at Test Time*, <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2501.00663"><u>https://arxiv.org/abs/2501.00663</u></a>



<hr>
*This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my *<a target="_blank" rel="" href="https://amzn.to/4fqvn0D">*<u>Build a Large Language Model (From Scratch) book</u>*</a>*. (I am confident that you'll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)*
<img class="rounded-lg border my-4" src="https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png" alt="" title="" width="1456" height="819">
<a target="_blank" rel="" href="https://amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167">*<u>Build a Large Language Model (From Scratch)</u>*</a>* now available on Amazon*

*If you read the book and have a few minutes to spare, I'd really appreciate a *<a target="_blank" rel="" href="https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167">*<u>brief review</u>*</a>*. It helps us authors a lot!*

Alternatively, I also recently enabled the paid subscription option on Substack to support this magazine directly.
